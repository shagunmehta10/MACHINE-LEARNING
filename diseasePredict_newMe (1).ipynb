{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36304ab-3090-475a-81be-5570e7f86950",
   "metadata": {},
   "source": [
    "# Step 1 ) Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecb3596-1303-40ba-98da-04d48b7164bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Shagun\\AppData\\Local\\Temp\\ipykernel_4960\\491354133.py\", line 7, in <module>\n",
      "    from sklearn.model_selection import train_test_split\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 84, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 11, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 8, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 11, in <module>\n",
      "    from scipy.sparse import csr_matrix, issparse\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\Shagun\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:295\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "#import Necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524bcdab-283b-4a9c-9d8f-c2444029d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\rahul\\\\Downloads\\\\symbipredict_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32530fd2-d2aa-4d74-92dd-fe5151040061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Data Information:\\n\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfb94e-d737-4bb3-955a-47a0490038db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "\n",
    "# 1) data.isnull().sum() :  calculates the number of missing values in each column.\n",
    "# 2) missing_values[missing_values > 0] : filters to display only columns with missing values.\n",
    "\n",
    "#This code will output a list of columns that have missing values and the number of missing values in each, if any.\n",
    "\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values[missing_values>0])\n",
    "\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"Number of Duplicate Rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeea871-902e-48d3-8265-0b00d4840493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows if any\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "#This line removes any duplicate rows from the dataset.\n",
    "#By reassigning data = data.drop_duplicates(), we are  ensuring that data is updated without duplicates.\n",
    "\n",
    "# Fill missing values with 0 (assuming missing values imply absence of symptom)\n",
    "\n",
    "\n",
    "#This fills all missing values with 0, assuming that missing values indicate the absence of a symptom.\n",
    "#We could also use other values (like the mean, median, or mode) if that makes more sense for our data, but 0 is reasonable for binary symptom data.\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Verify no missing values remain\n",
    "\n",
    "#This checks for any remaining missing values in the entire dataset.\n",
    "#data.isnull().sum().sum() gives the total count of missing values across all columns and rows. If this outputs 0, it confirms that there are no missing values left.\n",
    "\n",
    "\n",
    "print(\"Remaining Missing Values:\\n\", data.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d856948-bab8-45ca-b607-fa25aff2e3f7",
   "metadata": {},
   "source": [
    "# Step 2 ) Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7506a1-b36c-40ae-a531-7aa89a4f86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cdb18-6015-423f-b1a1-b2a5444d1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jupyter displays the head() function output as a structured table by default, providing a nice view of rows and columns.\n",
    "#This is automatic when you call data.head() without using print()\n",
    "\n",
    "# keep the structured table display, simply call data.head() without print()\n",
    "##Wrapping data.head() in print() outputs it as plain text, without the table formatting. This leads to a less structured display, with text-based row and column formatting.\n",
    "#This output appears more like raw data in text form.\n",
    "\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9ccaf-51a1-4985-b60f-c79de64e0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.describe() : to get a summary of the numerical data, which includes count, mean, standard deviation, min, and max values.\n",
    "# help understand the range and distribution of each symptom.\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee78695-181d-4695-8876-e2337af717d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['prognosis'].value_counts())  # Count of each prognosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbda677-4734-40f7-87f7-6949adc7f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of unique target classes (prognosis):\n",
    "unique_prognoses = data['prognosis'].nunique()\n",
    "print(f\"Number of unique prognoses: {unique_prognoses}\")\n",
    "print('\\n')\n",
    "#Listing all unique target classes (prognosis):\n",
    "prognoses_list = data['prognosis'].unique()\n",
    "print(\"Unique prognoses:\\n\")\n",
    "print(prognoses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff05a3-5c61-4173-bc1f-46a8ce554356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the distribution of target classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='prognosis', data=data, palette=\"Set2\")  \n",
    "plt.title('Distribution of Prognosis')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Prognosis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90309761-f191-47aa-bd59-0eeee219d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of your columns\n",
    "print(data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797d529-6c0b-41bc-b6c1-0ed6c01d44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'prognosis' column from the dataset\n",
    "numeric_data = data.drop(columns=['prognosis'])\n",
    "\n",
    "# Calculate the correlation matrix for the numerical features\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Adjust the size as needed\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Symptoms')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be125312-18ac-4327-9c49-eaeae6a43d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare data \n",
    "X = data.drop('prognosis', axis=1)  # Features\n",
    "y = data['prognosis']  # Target\n",
    "\n",
    "# Step 2: Train a Random Forest model to get feature importance\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Step 3: Calculate and display feature importance\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to hold feature names and their importance scores\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top 40 important features\n",
    "print(\"Top 40 Important Features:\")\n",
    "print(feature_importance.head(40))\n",
    "\n",
    "# Step 4: Visualize the top 30 feature importance with more distance between features\n",
    "plt.figure(figsize=(14, 12))  # Increase height for more space between the bars\n",
    "\n",
    "# Plot the top 30 features with horizontal bars\n",
    "feature_importance.head(30).plot(kind='barh', x='Feature', y='Importance', legend=False, color='skyblue')\n",
    "\n",
    "# Adjust the font sizes for better clarity\n",
    "plt.xlabel('Importance Score', fontsize=14)\n",
    "plt.ylabel('Features', fontsize=14)\n",
    "plt.title('Top 30 Feature Importance', fontsize=16)\n",
    "\n",
    "# Rotate the y-axis labels to prevent overlap (set to 0 degrees for horizontal)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "# Increase the spacing between the bars\n",
    "plt.subplots_adjust(left=0.2, right=0.95, top=0.95, bottom=0.05)\n",
    "\n",
    "# Adjust layout to ensure everything fits well without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Reverse the y-axis to show the most important features at the top\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327285a-9601-40bf-8027-1a874a6fecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size and adjust the rotation of labels\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Rotate labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Title for the heatmap\n",
    "plt.title('Correlation Matrix of Symptoms')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f0753-1c64-4c52-a72f-b80b77278121",
   "metadata": {},
   "source": [
    "# Step 4: Random Forest Classifier on top 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44ae38-a20d-452c-9737-8d52d3adc820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select top 30 features based on feature importance\n",
    "top_30_features = feature_importance.head(30)['Feature']\n",
    "\n",
    "#  Prepare the data with only top 30 features\n",
    "X_top_30 = X[top_30_features]  # Select only the top 30 important features\n",
    "\n",
    "#  Split the data into training and testing sets (70% training, 30% testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top_30, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier with 100 trees\n",
    "rfc_top_30 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest model with the top 30 features\n",
    "rfc_top_30.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rfc_top_30.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "#  Accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 11b. Accuracy on the training set\n",
    "train_accuracy = rfc_top_30.score(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "#  Cross-validation scores (5-fold)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(rfc_top_30, X_train, y_train, cv=5)\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-validation score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "#  Classification report (precision, recall, F1-score)\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd5249-48a7-40be-ac09-137d35ce44ed",
   "metadata": {},
   "source": [
    "# Random Forest Classifier on top 40 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae87b89-7d4a-43bd-9f0b-dcdd04dd1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Prepare the data (assuming 'data' is your dataframe)\n",
    "X = data.drop('prognosis', axis=1)  # Features\n",
    "y = data['prognosis']  # Target variable\n",
    "\n",
    "# Step 2: Get feature importances from Random Forest\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X, y)\n",
    "\n",
    "# Get feature importances and sort them\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rfc.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 3: Select the top 40 important features\n",
    "top_40_features = feature_importances.head(40)['Feature']\n",
    "\n",
    "# Step 4: Prepare training and testing data with top 40 features\n",
    "X_selected = X[top_40_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 5: Train the Random Forest Classifier\n",
    "rfc_top_40 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "#rfc_top_40 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc_top_40.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred_top_40 = rfc_top_40.predict(X_test)\n",
    "\n",
    "# Step 7: Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_top_40)\n",
    "print(f\"Accuracy with top 40 features: {accuracy:.4f}\")\n",
    "\n",
    "# Step 8: Print Classification Report with zero_division parameter to avoid warnings\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_top_40, zero_division=0))  # Set zero_division=0 to handle undefined metrics\n",
    "\n",
    "# Cross-validation with fewer splits\n",
    "cv_scores = cross_val_score(rfc_top_40, X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d516af-8be7-43ad-ac70-c471a1666755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st time CNN TRIED with 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b4f4f-8e31-48cb-a36e-b0890669a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Prepare the data for CNN\n",
    "X_cnn = X_selected.values.reshape(-1, 40, 1)  # Reshape for 1D convolution\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_cnn = to_categorical(y_encoded)  # One-hot encode target variable\n",
    "\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Define the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(40, 1)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y_cnn.shape[1], activation='softmax')  # Output layer with softmax for multi-class classification\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the CNN model\n",
    "cnn_model.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Evaluate the CNN model\n",
    "cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test_cnn, verbose=0)[1]\n",
    "print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
    "\n",
    "# Step 5: Classification Report\n",
    "y_pred_cnn = cnn_model.predict(X_test_cnn).argmax(axis=1)\n",
    "y_test_true = y_test_cnn.argmax(axis=1)  # Convert back to label indices\n",
    "print(\"Classification Report (CNN):\")\n",
    "print(classification_report(y_test_true, y_pred_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3b71b-7bb8-4982-80ee-b34314051801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st time rnn tried with 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a911e1-6b83-4f64-b898-6a156ef6bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "# Step 1: Prepare the data for RNN (same as CNN)\n",
    "X_rnn = X_selected.values.reshape(-1, 40, 1)\n",
    "\n",
    "# Step 2: Define the RNN model\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(64, activation='relu', input_shape=(40, 1)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y_cnn.shape[1], activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the RNN model\n",
    "rnn_model.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "# Step 4: Evaluate the RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test_cnn, y_test_cnn, verbose=0)[1]\n",
    "print(f\"RNN Accuracy: {rnn_accuracy:.4f}\")\n",
    "\n",
    "# Step 5: Classification Report\n",
    "y_pred_rnn = rnn_model.predict(X_test_cnn).argmax(axis=1)\n",
    "print(\"Classification Report (RNN):\")\n",
    "print(classification_report(y_test_true, y_pred_rnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80fbc9-5459-4101-a1ef-619910937bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"for 40 features: \\n\")\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"CNN Accuracy: {cnn_accuracy * 100:.2f}%\")\n",
    "print(f\"RNN Accuracy: {rnn_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d033884-972e-48ff-9f22-90d5f5a50095",
   "metadata": {},
   "source": [
    "rfc_all = RandomForestClassifier(random_state=42)\n",
    "rfc_all.fit(X_train, y_train)\n",
    "y_pred_all = rfc_all.predict(X_test)\n",
    "\n",
    "# Accuracy and report for model with all features\n",
    "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
    "print(f\"Accuracy with all features: {accuracy_all:.4f}\")\n",
    "\n",
    "print(\"Classification Report for all features:\")\n",
    "print(classification_report(y_test, y_pred_all, zero_division=0))\n",
    "\n",
    "# Cross-validation with all features\n",
    "cv_scores_all = cross_val_score(rfc_all, X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(f\"Cross-validation scores (All Features): {cv_scores_all}\")\n",
    "print(f\"Mean cross-validation score (All Features): {cv_scores_all.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1851dd-5a66-4da0-a3fc-655d55f7c5de",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Prepare the data (assuming 'data' is your dataframe)\n",
    "X = data.drop('prognosis', axis=1)  # Features\n",
    "y = data['prognosis']  # Target variable\n",
    "\n",
    "# Step 2: Get feature importances from Random Forest\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X, y)\n",
    "\n",
    "# Get feature importances and sort them\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rfc.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 3: Select the top 40 important features\n",
    "top_40_features = feature_importances.head(40)['Feature']\n",
    "\n",
    "# Step 4: Prepare training and testing data with top 40 features\n",
    "X_selected = X[top_40_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_top_40, X_test_top_40 = X_train[top_40_features], X_test[top_40_features]\n",
    "\n",
    "# Step 5: Train the Random Forest Classifier with all features\n",
    "rfc_all = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rfc_all.fit(X_train, y_train)\n",
    "y_pred_all = rfc_all.predict(X_test)\n",
    "\n",
    "# Accuracy and report for model with all features\n",
    "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
    "print(f\"Accuracy with all features: {accuracy_all:.4f}\")\n",
    "\n",
    "print(\"Classification Report for all features:\")\n",
    "print(classification_report(y_test, y_pred_all, zero_division=0))\n",
    "\n",
    "# Cross-validation with all features\n",
    "cv_scores_all = cross_val_score(rfc_all, X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(f\"Cross-validation scores (All Features): {cv_scores_all}\")\n",
    "print(f\"Mean cross-validation score (All Features): {cv_scores_all.mean():.4f}\")\n",
    "\n",
    "# Step 6: Train the Random Forest Classifier with top 40 features\n",
    "rfc_top_40 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rfc_top_40.fit(X_train_top_40, y_train)\n",
    "y_pred_top_40 = rfc_top_40.predict(X_test_top_40)\n",
    "\n",
    "# Accuracy and report for model with top 40 features\n",
    "accuracy_top_40 = accuracy_score(y_test, y_pred_top_40)\n",
    "print(f\"Accuracy with top 40 features: {accuracy_top_40:.4f}\")\n",
    "\n",
    "print(\"Classification Report for top 40 features:\")\n",
    "print(classification_report(y_test, y_pred_top_40, zero_division=0))\n",
    "\n",
    "# Cross-validation with top 40 features\n",
    "cv_scores_top_40 = cross_val_score(rfc_top_40, X_train_top_40, y_train, cv=3, scoring='accuracy')\n",
    "print(f\"Cross-validation scores (Top 40 Features): {cv_scores_top_40}\")\n",
    "print(f\"Mean cross-validation score (Top 40 Features): {cv_scores_top_40.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3950b-b0a5-42fc-bd90-5c43638fece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Step 1: Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_top_40)\n",
    "\n",
    "# Ensure that the labels match the unique classes in the prediction\n",
    "unique_labels = sorted(set(y_test) | set(y_pred_top_40))  # Unique labels from both y_test and y_pred\n",
    "\n",
    "# Display the confusion matrix\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=unique_labels)\n",
    "cmd.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix for Top 40 Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06a504-72f5-40d7-8c75-e56b5a144c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Building and Training a CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f502a88-762c-4152-a92e-80b07d5199e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data to ensure better performance with neural networks\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data to a \"pseudo-image\" format: (samples, rows, columns, channels)\n",
    "# Here we reshape each sample to 1x132 \"image\" with 1 channel (for simplicity)\n",
    "X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)  # (samples, features, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5950ef-4504-445f-be9b-3d622a361a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Initialize the CNN model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# Add the first convolutional layer\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_scaled.shape[1], 1)))\n",
    "\n",
    "# Add a max pooling layer\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer\n",
    "cnn_model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Output layer with softmax activation (for multi-class classification)\n",
    "cnn_model.add(Dense(units=len(np.unique(y)), activation='softmax'))  # Number of classes\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6731c4-c650-44f6-94c8-16b2bc377c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['prognosis'])\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Normalizing feature data\n",
    "X = data.drop(columns=['prognosis'])\n",
    "X_normalized = X / X.max()\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshaping input for CNN\n",
    "X_train_cnn = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Building the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "cnn_history = cnn_model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=20, batch_size=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481c0bf-4311-43d6-8a43-3d1dd18e092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries for RNN\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Reshaping input for RNN\n",
    "X_train_rnn = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_rnn = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Building the RNN model\n",
    "rnn_model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(1, X_train_rnn.shape[2])),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "rnn_history = rnn_model.fit(X_train_rnn, y_train, validation_data=(X_test_rnn, y_test), epochs=20, batch_size=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2cf67-b8be-4d11-a55c-9de7fc3d2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test)\n",
    "print(f\"CNN Accuracy: {cnn_accuracy*100:.2f}%\")\n",
    "\n",
    "# Evaluate RNN model\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test_rnn, y_test)\n",
    "print(f\"RNN Accuracy: {rnn_accuracy*100:.2f}%\")\n",
    "\n",
    "# Compare with Random Forest (from earlier work)\n",
    "print(\"Include your Random Forest metrics here for comparison.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00039791-d1cb-4218-b0e5-35389b64e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['prognosis'])\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Normalize feature data\n",
    "X = data.drop(columns=['prognosis'])\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets (with stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_onehot, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef2ed8-5800-4cf5-a225-9dcfdd113cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for CNN\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    validation_data=(X_test_cnn, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test)\n",
    "print(f\"CNN Accuracy: {cnn_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dcf3b-cba5-4f35-aee0-5b009a22cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for RNN\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Build RNN model with LSTM\n",
    "rnn_model = Sequential([\n",
    "    Bidirectional(LSTM(64, activation='relu', return_sequences=False), input_shape=(1, X_train.shape[1])),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train RNN model\n",
    "rnn_history = rnn_model.fit(\n",
    "    X_train_rnn, y_train,\n",
    "    validation_data=(X_test_rnn, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate RNN model\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test_rnn, y_test)\n",
    "print(f\"RNN Accuracy: {rnn_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a6197-dfa9-475f-9157-f08ba002b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "print(f\"Random Forest Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"CNN Accuracy: {cnn_accuracy*100:.2f}%\")\n",
    "print(f\"RNN Accuracy: {rnn_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834cc7a-e18d-474e-9989-02d5931e6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot CNN training history\n",
    "plt.plot(cnn_history.history['accuracy'], label='CNN Train Accuracy')\n",
    "plt.plot(cnn_history.history['val_accuracy'], label='CNN Validation Accuracy')\n",
    "plt.title('CNN Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot RNN training history\n",
    "plt.plot(rnn_history.history['accuracy'], label='RNN Train Accuracy')\n",
    "plt.plot(rnn_history.history['val_accuracy'], label='RNN Validation Accuracy')\n",
    "plt.title('RNN Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba810cbe-f7ed-4c48-b4a2-b7fb56193604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5323e-fb5f-4e02-a818-eff8308a8f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90126560-8898-4ff0-bb54-8ac9e141a0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016350d8-9932-47df-b4ef-7062000417a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4aee4-baa1-459c-bd87-3c185604cc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b1752-d6bf-470f-a676-e2a9e9a00bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeea288-4cb0-47fb-83c3-9e73dbc8392e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37435d92-2cc6-484f-87ca-642a8e6728af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3d5c5-65b5-4ce7-8440-0790eaa66377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f25f2-3260-43f8-81d0-1b00cd047eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming data is already loaded in 'X' (features) and 'y' (labels)\n",
    "\n",
    "# Step 1: Reshape data for RNN (since it's not time-series, we add 1 timestep)\n",
    "X_reshaped = X.values.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Step 2: Encode the target variable\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Step 3: Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(np.unique(y)), activation='softmax'))  # Multi-class classification\n",
    "\n",
    "# Step 5: Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 7: Make predictions and evaluate the model\n",
    "y_pred_rnn = model.predict(X_test)\n",
    "y_pred_rnn = np.argmax(y_pred_rnn, axis=1)  # Convert one-hot output to class index\n",
    "\n",
    "# Step 8: Evaluate model\n",
    "accuracy_rnn = accuracy_score(y_test, y_pred_rnn)\n",
    "print(\"\\nRNN Accuracy:\", accuracy_rnn)\n",
    "print(\"\\nClassification Report (RNN):\")\n",
    "print(classification_report(y_test, y_pred_rnn))\n",
    "\n",
    "# Step 9: Confusion Matrix for RNN\n",
    "print(\"\\nConfusion Matrix (RNN):\")\n",
    "cm_rnn = confusion_matrix(y_test, y_pred_rnn)\n",
    "sns.heatmap(cm_rnn, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix with RNN')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d9038-d73f-4e99-b974-5786275597b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Reshape the data for CNN (Add a channel dimension)\n",
    "X_reshaped_cnn = X.values.reshape((X.shape[0], X.shape[1], 1))  # Add a channel dimension\n",
    "\n",
    "# Step 2: Encode the target variable\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Step 3: Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped_cnn, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build the CNN model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dropout(0.2))\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(len(np.unique(y)), activation='softmax'))  # Multi-class classification\n",
    "\n",
    "# Step 5: Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history_cnn = model_cnn.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 7: Make predictions and evaluate the model\n",
    "y_pred_cnn = model_cnn.predict(X_test)\n",
    "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)  # Convert one-hot output to class index\n",
    "\n",
    "# Step 8: Evaluate model\n",
    "accuracy_cnn = accuracy_score(y_test, y_pred_cnn)\n",
    "print(\"\\nCNN Accuracy:\", accuracy_cnn)\n",
    "print(\"\\nClassification Report (CNN):\")\n",
    "print(classification_report(y_test, y_pred_cnn))\n",
    "\n",
    "# Step 9: Confusion Matrix for CNN\n",
    "print(\"\\nConfusion Matrix (CNN):\")\n",
    "cm_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix with CNN')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bc546-08f3-4cd6-8e8b-49f1c4b050cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
